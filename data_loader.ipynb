{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7325665-65a5-4675-83ad-b4959a75b8d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Date : June 15  \n",
    "Author : Ashok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c2d22-dcaa-474b-9def-cc25101e59dd",
   "metadata": {},
   "source": [
    "The following three cells are taken from the \"Carroll4th Cloud Coverage Data-Copy1.ipynb\" present [here](https://github.com/floodnet-nyc/Solar-Coverage-Data/blob/main/Carroll4th%20Cloud%20Coverage%20Data-Copy1.ipynb) but are modified to query data for any number of days one wants to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1df3de-b72a-4975-be4b-fe3c191f4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.ticker\n",
    "import matplotlib.transforms as transforms\n",
    "import matplotlib.patches as mpatches\n",
    "from influxdb_client import InfluxDBClient, Point\n",
    "from influxdb_client.client.write.retry import WritesRetry\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "import geocoder\n",
    "import glob\n",
    "from influxdb_client.domain.write_precision import WritePrecision\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(18,10)})\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db8c463-aa0c-4ca4-b393-481d3ee10b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_from_influxdb(_url=None,\n",
    "                        _token=None,\n",
    "                        _org=None,\n",
    "                        _id=None,\n",
    "                        _bucket_name=None,\n",
    "                        _columns_to_drop=None,\n",
    "                        _measurement=None,\n",
    "                        _field=None,\n",
    "                       _start = None):\n",
    "\n",
    "    \"\"\"\n",
    "        Function to query InfluxDB using influxdb-python-client library from https://github.com/influxdata/influxdb-client-python\n",
    "            :param _measurement: Available options are flood-sensor-tidal-sensor-rain-gauge,weather,etc. Default is flood-sensor\n",
    "            :param _field: Available options include batt_v and cloud_percent\n",
    "            :param _url: InfluxDB url\n",
    "            :param _token: API token with read-access\n",
    "            :param _org: InfluxDB organization name\n",
    "            :param _id: Sensor/Deployment ID\n",
    "            :param _bucket_name: Name of the bucket\n",
    "            :param _columns_to_drop: Array-like column names to be dropped\n",
    "            :param _start : Start time for the query,type : datetime\n",
    "        :returns a dataframe with all columns except dropped ones if any\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    p = {\"_id_value\": _id,\n",
    "         \"_bucket_name\": _bucket_name,\n",
    "         \"_measurement\": _measurement,\n",
    "         \"_field\": _field,\n",
    "        \"_start\": _start\n",
    "        }\n",
    "\n",
    "\n",
    "    with InfluxDBClient(url=_url, token=_token, org=_org, timeout=1000000, verify_ssl = False) as client:\n",
    "        query_api = client.query_api()\n",
    "        if _measurement==\"weather\":\n",
    "            result = query_api.query_data_frame('''from(bucket: _bucket_name)\n",
    "                                                  |> range(start: _start, stop: now())\n",
    "                                                  |> filter(fn: (r) => r[\"_measurement\"] == _measurement)\n",
    "                                                  |> filter(fn: (r) => r[\"_field\"] == _field)\n",
    "                                                  |> filter(fn: (r) => r[\"sensor_id\"] == _id_value)\n",
    "                                                  |> pivot(rowKey: [\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "                                                ''', params=p)\n",
    "        else:\n",
    "            result = query_api.query_data_frame('''from(bucket: _bucket_name)\n",
    "                                                |> range(start: _start)\n",
    "                                                |> filter(fn: (r) => r[\"_measurement\"] == _measurement)\n",
    "                                                |> filter(fn: (r) => r[\"deployment_id\"] == _id_value)\n",
    "                                                |> pivot(rowKey: [\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "                                                ''', params=p)\n",
    "\n",
    "        if type(result) == list:\n",
    "            result = pd.concat(result)\n",
    "\n",
    "    result.drop(columns=['_start', '_stop', 'result', 'table'], inplace=True) #result\n",
    "    result.rename(columns={'_time': 'time', '_measurement': 'measurement'}, inplace=True)\n",
    "    result['time'] = pd.to_datetime(result['time'], format=\"%Y-%m-%d %H:%M:%S.%f\", utc=True)\n",
    "    result.set_index('time', inplace=True)\n",
    "    result = result.sort_values(by=['time'])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54057d17-93b2-441f-a5f2-ec1967c24989",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saveddaily_gentle_beetle.csv\n",
      "savedoverly_heroic_squid.csv\n",
      "savedweekly_poetic_guinea.csv\n",
      "savedvastly_saving_whale.csv\n",
      "savedopenly_driven_tarpon.csv\n",
      "savedjolly_tender_squid.csv\n",
      "saveddaily_new_falcon.csv\n",
      "savedgladly_mint_snail.csv\n",
      "savedeasily_dear_mouse.csv\n",
      "savedbored_blue_fish.csv\n",
      "saveddaily_happy_satyr.csv\n",
      "savedmean_flying_fish.csv\n",
      "savedclosed_wagon_snail.csv\n",
      "savedclearly_bored_turtle.csv\n",
      "savedblue_eyed_tiger.csv\n",
      "savedsimply_half_monkey.csv\n",
      "savedtotal_melt_deer.csv\n",
      "savedlight_maroon_penguin.csv\n",
      "savedlovely_helped_lamb.csv\n",
      "saveddaily_ace_bear.csv\n"
     ]
    }
   ],
   "source": [
    "start_day = 30 ##number of days for which you want to get the data \n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Calculate the date and time exactly start_days days ago\n",
    "start_time = current_datetime - timedelta(days = start_day)\n",
    "\n",
    "# Getting the deployment id\n",
    "sensor_location_data = pd.read_csv(\"Sensor Locations Directory - Internal - street sensors (live).csv\")\n",
    "\n",
    "\n",
    "## loading token from secret.json\n",
    "with open('secrets.json', 'r') as file:\n",
    "    file_contents = json.load(file)\n",
    "    token = file_contents['influx_db']['token']\n",
    "    \n",
    "\n",
    "for deployment_id in sensor_location_data['deployment_id']:\n",
    "    try:\n",
    "        retrived_df= query_from_influxdb(_url='https://influxdb.floodlabs.nyc/',\n",
    "                           _token=token,\n",
    "                       _org='floodnet',\n",
    "                       _id= deployment_id,\n",
    "                      _bucket_name='floodnet-live',\n",
    "                       _measurement=\"flood-sensor\",\n",
    "                        _start = start_time,\n",
    "                       _field=\"batt_v\",).reset_index()[['time', 'batt_v', 'depth_filt_mm', 'dist_mm', 'f_cnt', 'gw_1_rssi_dbm', 'gw_1_snr_db','gw_2_rssi_dbm', 'gw_2_snr_db','gw_3_rssi_dbm', 'gw_3_snr_db','deployment_id']]\n",
    "\n",
    "\n",
    "        retrived_df['times'] = retrived_df['time'].dt.time\n",
    "        retrived_df['date'] = retrived_df['time'].dt.date\n",
    "\n",
    "        file_path = deployment_id + \".csv\"\n",
    "        retrived_df.to_csv(file_path)\n",
    "        print(\"saved\" + file_path)\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4fec13-4a76-4a3b-9a65-88ab3a325134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "floodnet",
   "language": "python",
   "name": "floodnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
