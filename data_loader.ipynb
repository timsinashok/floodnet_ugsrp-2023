{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7325665-65a5-4675-83ad-b4959a75b8d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Date : June 15  \n",
    "Author : Ashok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c2d22-dcaa-474b-9def-cc25101e59dd",
   "metadata": {},
   "source": [
    "The following three cells are taken from the \"Carroll4th Cloud Coverage Data-Copy1.ipynb\" present [here](https://github.com/floodnet-nyc/Solar-Coverage-Data/blob/main/Carroll4th%20Cloud%20Coverage%20Data-Copy1.ipynb) but are modified to query data for any number of days one wants to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1df3de-b72a-4975-be4b-fe3c191f4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.ticker\n",
    "import matplotlib.transforms as transforms\n",
    "import matplotlib.patches as mpatches\n",
    "from influxdb_client import InfluxDBClient, Point\n",
    "from influxdb_client.client.write.retry import WritesRetry\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "import geocoder\n",
    "import glob\n",
    "from influxdb_client.domain.write_precision import WritePrecision\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(18,10)})\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59eaf261-4744-4eda-ae33-d4a9734e0988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_from_influxdb(_url=None,\n",
    "                        _token=None,\n",
    "                        _org=None,\n",
    "                        _id=None,\n",
    "                        _bucket_name=None,\n",
    "                        _columns_to_drop=None,\n",
    "                        _measurement=None,\n",
    "                        _field=None,\n",
    "                       _start = None):\n",
    "    \n",
    "    \"\"\"\n",
    "        Function to query InfluxDB using influxdb-python-client library from https://github.com/influxdata/influxdb-client-python\n",
    "            :param _measurement: Available options are flood-sensor-tidal-sensor-rain-gauge,weather,etc. Default is flood-sensor\n",
    "            :param _field: Available options include batt_v and cloud_percent \n",
    "            :param _url: InfluxDB url\n",
    "            :param _token: API token with read-access\n",
    "            :param _org: InfluxDB organization name\n",
    "            :param _id: Sensor/Deployment ID\n",
    "            :param _bucket_name: Name of the bucket\n",
    "            :param _columns_to_drop: Array-like column names to be dropped\n",
    "        :returns a dataframe with all columns except dropped ones if any\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    p = {\"_id_value\": _id,\n",
    "         \"_bucket_name\": _bucket_name,\n",
    "         \"_measurement\": _measurement, \n",
    "         \"_field\": _field,\n",
    "        \"_start\" : _start }\n",
    "\n",
    "\n",
    "    with InfluxDBClient(url=_url, token=_token, org=_org, timeout=1000000,  verify_ssl = False) as client:\n",
    "        query_api = client.query_api()\n",
    "        if _measurement==\"weather\":  \n",
    "            result = query_api.query_data_frame('''from(bucket: _bucket_name)\n",
    "                                                  |> range(start: _start, stop: now())\n",
    "                                                  |> filter(fn: (r) => r[\"_measurement\"] == _measurement)\n",
    "                                                  |> filter(fn: (r) => r[\"_field\"] == _field)\n",
    "                                                  |> filter(fn: (r) => r[\"sensor_id\"] == _id_value)\n",
    "                                                  |> pivot(rowKey: [\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "                                                ''', params=p)   \n",
    "        else:        \n",
    "            result = query_api.query_data_frame('''from(bucket: _bucket_name)\n",
    "                                                |> range(start: _start)       \n",
    "                                                |> filter(fn: (r) => r[\"_measurement\"] == _measurement)\n",
    "                                                |> filter(fn: (r) => r[\"deployment_id\"] == _id_value)\n",
    "                                                |> pivot(rowKey: [\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "                                                ''', params=p)\n",
    "\n",
    "        if type(result) == list:\n",
    "            result = pd.concat(result)\n",
    "    print(result.columns)\n",
    "    result.drop(columns=['_start', '_stop', 'result', 'table'], inplace=True) #result\n",
    "    result.rename(columns={'_time': 'time', '_measurement': 'measurement'}, inplace=True)\n",
    "    result['time'] = pd.to_datetime(result['time'], format=\"%Y-%m-%d %H:%M:%S.%f\", utc=True)\n",
    "    result.set_index('time', inplace=True)\n",
    "    result = result.sort_values(by=['time'])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164ddf3-7bd9-49cd-836d-dc5cc1ed2e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['result', 'table', '_start', '_stop', '_time', '_measurement',\n",
      "       'app_name', 'bw_hz', 'coding_rate', 'deployment_id', 'dev_addr',\n",
      "       'dev_eui', 'dev_id', 'error_flag', 'f_port', 'gw_1_id', 'network', 'sf',\n",
      "       'airtime_s', 'batt_v', 'depth_filt_mm', 'depth_filt_stages_applied',\n",
      "       'depth_proc_dist_mm', 'depth_proc_mm', 'depth_raw_mm', 'dist_mm',\n",
      "       'f_cnt', 'flood_detected', 'gw_1_rssi_dbm', 'gw_1_snr_db',\n",
      "       'lora_freq_hz', 'gw_2_id', 'gw_3_id', 'gw_2_rssi_dbm', 'gw_2_snr_db',\n",
      "       'gw_3_rssi_dbm', 'gw_3_snr_db', 'gw_4_id', 'gw_4_rssi_dbm',\n",
      "       'gw_4_snr_db', 'gw_5_id', 'gw_5_rssi_dbm', 'gw_5_snr_db', 'gw_6_id',\n",
      "       'gw_6_rssi_dbm', 'gw_6_snr_db'],\n",
      "      dtype='object')\n",
      "saveddaily_gentle_beetle.csv\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "start_day = 30 ##number of days for which you want to get the data \n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Calculate the date and time exactly start_days days ago\n",
    "start_time = current_datetime - timedelta(days = start_day)\n",
    "\n",
    "# Getting the deployment id\n",
    "sensor_location_data = pd.read_csv(\"Sensor Locations Directory - Internal - street sensors (live).csv\")\n",
    "\n",
    "\n",
    "interested_fields = ['time', 'deployment_id', 'error_flag','batt_v', 'gw_1_id', 'dist_mm', 'f_cnt', 'gw_1_rssi_dbm', 'gw_1_snr_db']\n",
    "retrived_df = pd.DataFrame()\n",
    "\n",
    "## loading token from secret.json\n",
    "with open('secrets.json', 'r') as file:\n",
    "    file_contents = json.load(file)\n",
    "    token = file_contents['influx_db']['token']\n",
    "    \n",
    "failed_data_access = []\n",
    "    \n",
    "for deployment_id in sensor_location_data['deployment_id'][1:]:\n",
    "    try:\n",
    "        retrived_df= query_from_influxdb(_url='https://influxdb.floodlabs.nyc/',\n",
    "                           _token=token,\n",
    "                       _org='floodnet',\n",
    "                       _id= deployment_id,\n",
    "                      _bucket_name='floodnet-live',\n",
    "                       _measurement=\"flood-sensor\",\n",
    "                        _start = start_time,\n",
    "                       _field=\"batt_v\",).reset_index()[interested_fields]\n",
    "\n",
    "\n",
    "        retrived_df['times'] = retrived_df['time'].dt.time\n",
    "        retrived_df['date'] = retrived_df['time'].dt.date\n",
    "        file_path = deployment_id + \".csv\"\n",
    "        retrived_df.to_csv(file_path)\n",
    "        print(\"saved\" + file_path)\n",
    "    except:\n",
    "        failed_data_access.append(deployment_id)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ab8ce9-6db1-4357-9133-4a702bc8607c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['widely_proud_lizard',\n",
       " 'early_still_frog',\n",
       " 'really_vocal_puma',\n",
       " 'evenly_divine_dingo',\n",
       " 'daily_mutual_gnat',\n",
       " 'easily_cosmic_slug',\n",
       " 'simply_pet_joey',\n",
       " 'mainly_fond_boar',\n",
       " 'disabled_pink_prawn',\n",
       " '(not yet live)',\n",
       " 'mostly_ample_newt',\n",
       " 'early_calm_ghoul',\n",
       " 'deadly_trusty_troll',\n",
       " 'humbly_modest_vervet']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_data_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075545e0-72c1-4ed1-b691-371916a7ebf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "floodnet",
   "language": "python",
   "name": "floodnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
